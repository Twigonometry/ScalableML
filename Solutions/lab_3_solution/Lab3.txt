Conda uses environments to load different sets of Python packages
type conda env list to see the environments availible.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/03/07 11:01:50 INFO SparkContext: Running Spark version 3.2.1
22/03/07 11:01:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/03/07 11:01:51 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
22/03/07 11:01:51 INFO ResourceUtils: ==============================================================
22/03/07 11:01:51 INFO ResourceUtils: No custom resources configured for spark.driver.
22/03/07 11:01:51 INFO ResourceUtils: ==============================================================
22/03/07 11:01:51 INFO SparkContext: Submitted application: Lab 4 Exercise
22/03/07 11:01:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/03/07 11:01:51 INFO ResourceProfile: Limiting resource is cpu
22/03/07 11:01:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
22/03/07 11:01:52 INFO SecurityManager: Changing view acls to: your_username
22/03/07 11:01:52 INFO SecurityManager: Changing modify acls to: your_username
22/03/07 11:01:52 INFO SecurityManager: Changing view acls groups to: 
22/03/07 11:01:52 INFO SecurityManager: Changing modify acls groups to: 
22/03/07 11:01:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(your_username); groups with view permissions: Set(); users  with modify permissions: Set(your_username); groups with modify permissions: Set()
22/03/07 11:01:53 INFO Utils: Successfully started service 'sparkDriver' on port 37452.
22/03/07 11:01:53 INFO SparkEnv: Registering MapOutputTracker
22/03/07 11:01:53 INFO SparkEnv: Registering BlockManagerMaster
22/03/07 11:01:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/03/07 11:01:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/03/07 11:01:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/03/07 11:01:53 INFO DiskBlockManager: Created local directory at /mnt/fastdata/your_username/blockmgr-5beb9ee0-a44b-4809-ac6f-274c9c22d3f7
22/03/07 11:01:53 INFO MemoryStore: MemoryStore started with capacity 408.9 MiB
22/03/07 11:01:53 INFO SparkEnv: Registering OutputCommitCoordinator
22/03/07 11:01:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/03/07 11:01:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://sharc-node173.shef.ac.uk:4040
22/03/07 11:01:54 INFO Executor: Starting executor ID driver on host sharc-node173.shef.ac.uk
22/03/07 11:01:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33913.
22/03/07 11:01:54 INFO NettyBlockTransferService: Server created on sharc-node173.shef.ac.uk:33913
22/03/07 11:01:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/03/07 11:01:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, sharc-node173.shef.ac.uk, 33913, None)
22/03/07 11:01:54 INFO BlockManagerMasterEndpoint: Registering block manager sharc-node173.shef.ac.uk:33913 with 408.9 MiB RAM, BlockManagerId(driver, sharc-node173.shef.ac.uk, 33913, None)
22/03/07 11:01:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, sharc-node173.shef.ac.uk, 33913, None)
22/03/07 11:01:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, sharc-node173.shef.ac.uk, 33913, None)
/home/your_username/.conda/envs/myspark/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.
  FutureWarning
22/03/07 11:01:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
22/03/07 11:01:56 INFO SharedState: Warehouse path is 'file:/data/your_username/ScalableML/HPC/spark-warehouse'.
22/03/07 11:02:04 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
L1 Accuracy = 0.913176 
5 most relevant features L1 features: 
char_freq_$
word_freq_remove
word_freq_000
word_freq_order
char_freq_!
Accuracy = 0.915461 
5 most relevant features L2 features: 
char_freq_$
word_freq_remove
word_freq_000
word_freq_table
word_freq_conference
Accuracy = 0.913176 
5 most relevant features Elastic Net features: 
char_freq_$
word_freq_remove
word_freq_000
word_freq_order
char_freq_!
22/03/07 11:04:45 WARN BlockManager: Asked to remove block broadcast_8616, which does not exist
22/03/07 11:05:14 WARN BlockManager: Asked to remove block broadcast_10179, which does not exist
Accuracy for best lm model = 0.923077 
{
    "aggregationDepth": 2,
    "elasticNetParam": 1.0,
    "family": "binomial",
    "featuresCol": "features",
    "fitIntercept": true,
    "labelCol": "labels",
    "maxBlockSizeInMB": 0.0,
    "maxIter": 25,
    "predictionCol": "prediction",
    "probabilityCol": "probability",
    "rawPredictionCol": "rawPrediction",
    "regParam": 0.001,
    "standardization": true,
    "threshold": 0.5,
    "tol": 1e-06
}
